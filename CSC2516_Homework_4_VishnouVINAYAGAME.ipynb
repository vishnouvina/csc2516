{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Backpropagation for bias vectors (1.5 points)\n",
        "\n",
        "In class, we discussed a multilayer perceptron (neural network) whose layers were all \"dense\", i.e. the output $a^m \\in \\mathbb{R}^{N^m}$ of the $m$th layer is computed as\n",
        "\\begin{align*}\n",
        "z^m &= W^m a^{m - 1} + b^m \\\\\n",
        "a^m &= \\sigma^m(z^m)\n",
        "\\end{align*}\n",
        "where $W^m \\in \\mathbb{R}^{N^m \\times N^{m - 1}}$ is the weight matrix, $b^m \\in \\mathbb{R}^{N^m}$ is the bias vector, and $\\sigma^m$ is the nonlinearity. We showed that\n",
        "$$\\frac{\\partial C}{\\partial W^m} = \\frac{\\partial C}{\\partial z^m} a^{m - 1 \\top}$$\n",
        "Show that\n",
        "$$\\frac{\\partial C}{\\partial b^m} = \\frac{\\partial C}{\\partial z^m}$$\n",
        "Hint: The derivation is almost the same as for $W$."
      ],
      "metadata": {
        "id": "nkK2eaK1SiNt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer 1.**\n",
        "\n",
        "By the chain rule, we have:\n",
        "$$\\frac{\\partial C}{\\partial b^m} = \\frac{\\partial C}{\\partial z^m}\\frac{\\partial z^m}{\\partial b^m}$$\n",
        "\n",
        "However, $$z^m = W^m a^{m - 1} + b^m $$ so\n",
        "$$\\frac{\\partial z^m}{\\partial b^m} = 1 $$\n",
        "and therefore : $$\\frac{\\partial C}{\\partial b^m} = \\frac{\\partial C}{\\partial z^m}$$\n"
      ],
      "metadata": {
        "id": "BKSX2f-X9xbw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. MLP from scratch (3.5 points)\n",
        "\n",
        "Using numpy only, implement backward pass or a sigmoid MLP. Specifically, you will need to implement this functionality in the `train` function in the `SigmoidMLP` class below. You should write numpy code to populate the two lists `weight_gradients` and `bias_gradient`, where each entry in each list corresponds to the gradient for a weight matrix or bias vector for each layer. Then, when you run the code cell at the bottom of this notebook, the trained MLP should output (approximately) 0, 1, 1, 0, having learned the [XOR function](https://en.wikipedia.org/wiki/Exclusive_or). Please us a binary cross-entropy loss, i.e.\n",
        "$$C(a^L, y) = (y - 1)\\log(1 - a^L) - y\\log(a^L)$$\n",
        "\n",
        "*Note 1*: All layers in your model, including the last layer, will use the sigmoid nonlinearity. Remember that\n",
        "$$\n",
        "\\frac{\\partial}{\\partial x}\\mathrm{sigmoid}(x) = \\mathrm{sigmoid}(x)(1 - \\mathrm{sigmoid}(x))$$\n",
        "\n",
        "*Note 2*: As we mentioned in class,\n",
        "$$\n",
        "\\frac{\\partial C}{\\partial z^L} = a^L - y\n",
        "$$"
      ],
      "metadata": {
        "id": "TerrZtS6T31p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class Layer:\n",
        "    def __init__(self, inputs, outputs):\n",
        "        # Initialize weight matrix and bias vector\n",
        "        # Getting the initialization right can be tricky, but for this problem\n",
        "        # simply drawing from a standard normal distribution should work.\n",
        "        self.weights = np.random.randn(outputs, inputs)* np.sqrt(1/inputs)\n",
        "        self.biases = np.random.randn(outputs, 1)\n",
        "    def __call__(self, X):\n",
        "        # Compute \\sigmoid(Wx + b)\n",
        "        return 1/(1 + np.exp(-(self.weights.dot(X) + self.biases)))\n",
        "\n",
        "class SigmoidMLP:\n",
        "\n",
        "    def __init__(self, layer_widths):\n",
        "        self.layers = []\n",
        "        for inputs, outputs in zip(layer_widths[:-1], layer_widths[1:]):\n",
        "            self.layers.append(Layer(inputs, outputs))\n",
        "\n",
        "    def train(self, inputs, targets, learning_rate):\n",
        "        # Forward pass - compute each layer's output and store it for later use\n",
        "        layer_outputs = [inputs]\n",
        "        for layer in self.layers:\n",
        "            layer_outputs.append(layer(layer_outputs[-1]))\n",
        "\n",
        "        # Implement backward pass to populate weight_gradients and bias_gradients\n",
        "        # lists here\n",
        "        weight_gradients = []\n",
        "        bias_gradients = []\n",
        "\n",
        "        residual = -((targets-1)/(1-layer_outputs[-1]) + (targets)/(layer_outputs[-1]))\n",
        "\n",
        "        for i in range(1,len(layer_outputs)):\n",
        "\n",
        "          if i == 1:\n",
        "            residual = residual*layer_outputs[-i]*(1-layer_outputs[-i])\n",
        "          else:\n",
        "            residual = np.matmul(self.layers[-i+1].weights.T, residual)*layer_outputs[-i]*(1-layer_outputs[-i])\n",
        "\n",
        "          bias_gradients.append(np.mean(residual, axis=1, keepdims=True))\n",
        "          weight_gradients.append(np.matmul(residual, layer_outputs[-i-1].T))\n",
        "\n",
        "        #we need to reverse the list as gradients are appended in the backprop order\n",
        "        bias_gradients = bias_gradients[::-1]\n",
        "        weight_gradients = weight_gradients[::-1]\n",
        "\n",
        "        # Perform gradient descent by applying updates\n",
        "        for weight_gradient, bias_gradient, layer in zip(weight_gradients, bias_gradients, self.layers):\n",
        "            layer.weights -= weight_gradient*learning_rate\n",
        "            layer.biases -= bias_gradient*learning_rate\n",
        "\n",
        "    def __call__(self, inputs):\n",
        "        a = inputs\n",
        "        for layer in self.layers:\n",
        "            a = layer(a)\n",
        "        return a\n",
        "\n",
        "def train_mlp(n_iterations, learning_rate):\n",
        "    mlp = SigmoidMLP([2, 2, 1])\n",
        "    inputs = np.array([[0, 1, 0, 1],\n",
        "                       [0, 0, 1, 1]])\n",
        "    targets = np.array([[0, 1, 1, 0]])\n",
        "    for _ in range(n_iterations):\n",
        "        mlp.train(inputs, targets, learning_rate)\n",
        "    return mlp"
      ],
      "metadata": {
        "id": "W-v0EeRsV8m9"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 0.8908817635270542\n",
        "n_iter = 1000\n",
        "mlp = train_mlp(n_iter, learning_rate)\n",
        "\n",
        "print(mlp(np.array([0, 0]).reshape(-1, 1)))\n",
        "print(mlp(np.array([0, 1]).reshape(-1, 1)))\n",
        "print(mlp(np.array([1, 0]).reshape(-1, 1)))\n",
        "print(mlp(np.array([1, 1]).reshape(-1, 1)))"
      ],
      "metadata": {
        "id": "yoAl7lW_WyNg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "26ce6844-89d3-498d-d196-7e5d4242756f"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.01982583]]\n",
            "[[0.98749758]]\n",
            "[[0.98749717]]\n",
            "[[0.02155465]]\n"
          ]
        }
      ]
    }
  ]
}